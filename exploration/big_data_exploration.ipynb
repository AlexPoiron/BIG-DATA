{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet BIG DATA : Partie Exploration\n",
    "POIRON Alex 23492 et THIL Tom 23034"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "#Manipulation to not confuse max() function from Python and max() function from Pyspark\n",
    "to_exclude = ['max', 'sum']\n",
    "from pyspark.sql.functions import *\n",
    "for name in to_exclude:\n",
    "    del globals()[name]\n",
    "\n",
    "from pyspark.sql.functions import max as max_\n",
    "from pyspark.sql.functions import sum as sum_\n",
    "\n",
    "#Correlation imports\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "#Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "from matplotlib import rcParams\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/21 15:50:12 WARN Utils: Your hostname, alex-ASUS resolves to a loopback address: 127.0.1.1; using 192.168.1.44 instead (on interface enx4ce1734b82ca)\n",
      "22/05/21 15:50:12 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/21 15:50:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark_application_name = \"Projet\"\n",
    "spark = (SparkSession.builder.appName(spark_application_name).getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      "\n",
      "+----------+-----------------+-----------------+-----------------+-----------------+-------+-----------------+------------+\n",
      "|       _c0|              _c1|              _c2|              _c3|              _c4|    _c5|              _c6|         _c7|\n",
      "+----------+-----------------+-----------------+-----------------+-----------------+-------+-----------------+------------+\n",
      "|      Date|             High|              Low|             Open|            Close| Volume|        Adj Close|company_name|\n",
      "|2017-01-03| 758.760009765625|747.7000122070312|757.9199829101562|753.6699829101562|3521100|753.6699829101562|      AMAZON|\n",
      "|2017-01-04|759.6799926757812|754.2000122070312|758.3900146484375|757.1799926757812|2510500|757.1799926757812|      AMAZON|\n",
      "|2017-01-05|782.4000244140625| 760.260009765625|761.5499877929688|780.4500122070312|5830100|780.4500122070312|      AMAZON|\n",
      "|2017-01-06|799.4400024414062|  778.47998046875|782.3599853515625| 795.989990234375|5986200| 795.989990234375|      AMAZON|\n",
      "|2017-01-09|  801.77001953125|  791.77001953125|            798.0|796.9199829101562|3446100|796.9199829101562|      AMAZON|\n",
      "|2017-01-10|            798.0|789.5399780273438|796.5999755859375|795.9000244140625|2558400|795.9000244140625|      AMAZON|\n",
      "|2017-01-11|            799.5| 789.510009765625|793.6599731445312|  799.02001953125|2992800|  799.02001953125|      AMAZON|\n",
      "|2017-01-12|814.1300048828125|            799.5|800.3099975585938|813.6400146484375|4873900|813.6400146484375|      AMAZON|\n",
      "|2017-01-13|821.6500244140625|811.4000244140625|814.3200073242188|817.1400146484375|3791900|817.1400146484375|      AMAZON|\n",
      "+----------+-----------------+-----------------+-----------------+-----------------+-------+-----------------+------------+\n",
      "\n",
      "Number of rows = 988\n"
     ]
    }
   ],
   "source": [
    "amazon = spark.read.csv(\"./stocks_data/AMAZON.csv\")\n",
    "amazon.printSchema()\n",
    "amazon.select('*').limit(10).show()\n",
    "print(\"Number of rows =\", amazon.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il nous faut changer le schema egalement de ces dataframe pour que nous puissions mieux nous servir des donnees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_columns = [StructField(\"Date\",TimestampType()), StructField(\"High\",FloatType()), StructField(\"Low\",FloatType()), StructField(\"Open\",FloatType()),StructField(\"Close\",FloatType()), StructField(\"Volume\",FloatType()), StructField(\"Adj Close\",FloatType()), StructField(\"company_name\",StringType())]\n",
    "new_schema = StructType(stocks_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque que l'on peut faire une fonction générique qui peut être appelée à chaque fois. Cela serait beucoup plus rapide si à chaque fois en plus de load le dataset, on souhaite avoir quelques informations dessus. C'est ce que nous allons faire maintenant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_infos(path):\n",
    "    \"\"\"\n",
    "        :param path : related path to the file\n",
    "        :return the new DataFrame created\n",
    "    \"\"\"\n",
    "    df_with_null = spark.read.csv(path, new_schema) #Creating the df with the new schema, we have a first null row\n",
    "    df = spark.createDataFrame(df_with_null.tail(df_with_null.count()-1), df_with_null.schema) #With this line we delete it\n",
    "    df.printSchema()\n",
    "    df.show(40)\n",
    "    print(\"Number of rows =\", df.count())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date period\n",
    "Nous allons maintenant nous intéresser à la période qui s'écoule entre chaque observation de chaque DataFrame. Pour ce faire, nous allons faire une fonction qui en prenant un DataFrame en paramètre, retournera le type de période éecartant chaque observation. (jour le jour, quelques jours, semaines, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_period(df):\n",
    "    \"\"\"\n",
    "        :param df the DataFrame\n",
    "        :return a string that correspond to the period in the DataFrame\n",
    "    \"\"\"\n",
    "    diff = []\n",
    "    name = df.first()[\"company_name\"]\n",
    "    dates = df.select('Date').rdd.flatMap(lambda x: x).collect()\n",
    "    #Day case\n",
    "    for i in range(0, len(dates)-1):\n",
    "        diff.append(dates[i+1].day - dates[i].day)\n",
    "    \n",
    "    period = max(diff, key=diff.count)\n",
    "    if period == 1 : \n",
    "        return \"For \" + name + \", it's a day period\"\n",
    "    \n",
    "    #Month and year case\n",
    "    diff = []\n",
    "    for i in range(0, len(dates)-1):\n",
    "        diff.append(dates[i+1].month - dates[i].month)\n",
    "    period = max(diff, key=diff.count)\n",
    "    \n",
    "    if period == 1 : \n",
    "        return \"For \" + name + \", it's a month period\"\n",
    "    else: \n",
    "        return \"For \" + name + \", its a year period\"        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive Statistics\n",
    "Fonction permettant d'avoir certaines statistiques sur chaque colonne d'un dataframe (min, max, moyenne, variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats(df):\n",
    "    \"\"\"\n",
    "        :param df : DataFrame\n",
    "        :return Print mean, min, max, sd for each column of the DataFrame\n",
    "    \"\"\"\n",
    "    columns = [\"High\", \"Low\", \"Open\", \"Close\", \"Volume\", \"Adj Close\"]\n",
    "    \n",
    "    for colname in columns:\n",
    "        print(\"Stats for :\", colname)\n",
    "        df.agg(mean(df[colname]), min(df[colname]), max_(df[colname]), stddev(df[colname])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check missing values\n",
    "Fonction qui permet de verifier s'il y a des valeurs nulles dans chaque colonnes du DataFrame donnee en parametre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_missing(df):\n",
    "    \"\"\"\n",
    "        :param df : DataFrame\n",
    "        :return : Show NULL values \n",
    "    \"\"\"\n",
    "    df.select(*[\n",
    "    (\n",
    "        count(when((isnan(c) | col(c).isNull()), c)) if t not in \"timestamp\"\n",
    "        else count(when(col(c).isNull(), c))\n",
    "    ).alias(c)\n",
    "    for c, t in df.dtypes if c in df.columns\n",
    "]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation\n",
    "Pour chaque DataFrame, nous allons étudier les corrélations entre les données. Pour ce faire, nous allons changer le type des donées en **Vector** puis appliquer dessus une fonction du package **ML** de pyspark pour avoir la matrice de corrélation. Nous crééons donc une fonction générique qui prend en paramètre un DataFrame et qui retourne sa matrice de corrélation associée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corr_matrix(df):\n",
    "    \"\"\"\n",
    "        :param df : DataFrame\n",
    "        :return the correlation matrix of the DataFrame\n",
    "    \"\"\"\n",
    "    vect_col = \"corr_data\"\n",
    "    df = df.drop('Date','company_name') #Only numeric values\n",
    "    assembler = VectorAssembler(inputCols=df.columns, outputCol=vect_col)\n",
    "    df_vector = assembler.transform(df).select(vect_col)\n",
    "    print(df.columns)\n",
    "\n",
    "    #Transform dataframe matrix in list by getting its values\n",
    "    matrix = Correlation.corr(df_vector, vect_col).collect()[0][0].toArray().tolist()\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions on the data\n",
    "\n",
    "- What is the average of the opening and closing prices for each stock price and for\n",
    "different time periods (week, month, year)\n",
    "\n",
    "On créé une fonction qui calcule la moyenne des prix selon une période précise. Pour nous aider, on créée également une fonction auxiliaire qui permet de construire une liste correspondant à la bonne temporalité demandée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_period(df, w=False, m=False, y=False):\n",
    "    \"\"\"\n",
    "        :param \n",
    "            - df : DataFrame\n",
    "            - w : Boolean True if we want a week period, False otherwise\n",
    "            - m : Boolean True if we want a month period, False otherwise\n",
    "            - y : Boolean True if we want a year period, False otherwise\n",
    "\n",
    "        :return : List containing date values corresponding to the period. \n",
    "            - [2017,2018,...] for years\n",
    "            - [1, 2, 3, 4, ..., 12] for months\n",
    "            - [1, 2, ..., 52] for weeks\n",
    "    \"\"\"\n",
    "    list = []\n",
    "    if w:\n",
    "        dates = df.select(weekofyear(df['Date'])).collect()\n",
    "        for i in range(len(dates)):\n",
    "            if dates[i][0] not in list:\n",
    "                list.append(dates[i][0])\n",
    "    else:\n",
    "        dates = df.select('Date').collect() #convert column to list\n",
    "        for date in dates:\n",
    "            if y:\n",
    "                if date[0].year not in list:\n",
    "                    list.append(date[0].year)\n",
    "            else:\n",
    "                if date[0].month not in list:\n",
    "                    list.append(date[0].month)\n",
    "    list.sort()\n",
    "    return list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_price(df, w=False, m=False, y=False):\n",
    "    \"\"\"\n",
    "        :param \n",
    "            - df : DataFrame\n",
    "            - w : Boolean True if we want a week period, False otherwise\n",
    "            - m : Boolean True if we want a month period, False otherwise\n",
    "            - y : Boolean True if we want a year period, False otherwise\n",
    "\n",
    "        :return : Show average for opening and closing price calculated following the period choose\n",
    "            - if year -> [2017, 2018, ...] -> average open and close price for 2017, 2018, ...\n",
    "            - if month -> [1, 2, ...] -> average open and clos eprice for 1, 2, 3, ...\n",
    "            - if week -> [1, 2, ..., 52] -> average open and clos eprice for 1, 2, 3, ..., 52\n",
    "    \"\"\"\n",
    "    list_period = get_list_period(df, w, m, y)\n",
    "    for i in list_period:\n",
    "        #Filter the Dataframe following the period choose\n",
    "        if y: df_filtered = df.filter(year(df['Date']) == i)\n",
    "        elif m: df_filtered = df.filter(month(df['Date']) == i)\n",
    "        else: df_filtered = df.filter(weekofyear(df['Date']) == i)\n",
    "        \n",
    "        print(\"Average open price and close price in\", i)\n",
    "        df_filtered.agg(mean(df_filtered['Open']), mean(df_filtered['Close'])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How do the stock prices change day to day and month to month (may be you can\n",
    "create new columns to save those calculations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_evolution_stock_prices(df, d=False,m=False):\n",
    "    \"\"\"\n",
    "        :param \n",
    "            - df : DataFrame\n",
    "            - w : Boolean True if we want a week period, False otherwise\n",
    "            - m : Boolean True if we want a month period, False otherwise\n",
    "        \n",
    "        :return Plotting evolution for each price for each DataFrame following a \n",
    "                specific period(each month or day to day). \n",
    "                For each DataFrame, we have a subplot with all prices except Volume price and another \n",
    "                subplot with Volume price. This separation is caused by the values of Volume price \n",
    "                that are higher than other prices\n",
    "    \"\"\"\n",
    "    #Construct list of y axis that is period scale (year, week or month)\n",
    "    period = []\n",
    "    if d:\n",
    "        dates = df.select(dayofyear(df['Date'])).collect()\n",
    "        for i in range(len(dates)):\n",
    "                period.append(dates[i][0])\n",
    "    else:\n",
    "        dates = df.select('Date').collect() #convert column to list\n",
    "        for date in dates:\n",
    "            if date[0].month not in period:\n",
    "                period.append(date[0].month)\n",
    "    \n",
    "    if m: period.sort() #sort for months not for days\n",
    "    \n",
    "    #Lists for all prices except volume\n",
    "    avg_open = []\n",
    "    avg_close = []\n",
    "    avg_high = []\n",
    "    avg_low = []\n",
    "    avg_adj_close = []\n",
    "    prices = [\n",
    "        (avg_open, \"Open\"),\n",
    "        (avg_close, \"Close\"),\n",
    "        (avg_high, \"High\"),\n",
    "        (avg_low, \"Low\"),\n",
    "        (avg_adj_close, \"Adj CLose\")\n",
    "        ]\n",
    "    \n",
    "    avg_volume = []\n",
    "\n",
    "    #Construct lists\n",
    "    for i in period:\n",
    "        if m: df_filtered = df.filter(month(df['Date']) == i)\n",
    "        else: df_filtered = df.filter(dayofyear(df['Date']) == i)\n",
    "\n",
    "        for (price, name_price) in prices:\n",
    "            price.append(df_filtered.agg(mean(df_filtered[name_price])).collect()[0][0])\n",
    "        \n",
    "        avg_volume.append(df_filtered.agg(mean(df_filtered['Volume'])).collect()[0][0])\n",
    "\n",
    "    \n",
    "    #PLotting\n",
    "    name = df.first()['company_name']\n",
    "    x = np.arange(len(period))\n",
    "    figure, axis = plt.subplots(1, 2, figsize=(10,7))\n",
    "    \n",
    "    #First plot for al prices except volume due to the diffence in values\n",
    "    for (price, name_price) in prices:\n",
    "        axis[0].plot(x, price, label=name_price)\n",
    "\n",
    "    axis[0].set_title('Average by prices except Volume one for ' + name)\n",
    "    axis[0].set_xlabel(\"Period of time\")\n",
    "    axis[0].set_ylabel('Average for all prices')\n",
    "    axis[0].legend()\n",
    "\n",
    "    #2d plot for volume prices\n",
    "    axis[1].plot(x, avg_volume, label=\"Volume price\")\n",
    "    axis[1].set_title('Average for Volume price for ' + name)\n",
    "    axis[1].legend()\n",
    "    axis[1].set_xlabel('Period of time')\n",
    "    \n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Based on the opening and closing price, calculate the daily return of each stock\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def daily_return(df):\n",
    "    \"\"\"\n",
    "        :param df : Dataframe\n",
    "        :return add in a column the daily return calculated day to day\n",
    "    \"\"\"\n",
    "    df.withColumn(\"evolution_price\", df['Close'] - df['Open']).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What are the stocks with the highest daily return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highest_daily_return():\n",
    "    \"\"\"\n",
    "        :return : the highest daily return\n",
    "    \"\"\"\n",
    "    maxi = 0\n",
    "    for df in LIST_DF:\n",
    "        tmp = df.agg(max_(df['Close'] - df['Open'])).collect()[0][0]\n",
    "        name_tmp = df.first()['company_name']\n",
    "        if (tmp > maxi): \n",
    "            maxi = tmp\n",
    "            name = name_tmp\n",
    "\n",
    "    print(\"Highest daily return is :\",maxi ,\"found in\", name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Calculate the average daily return for different periods (week, month, and year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_daily_return(df, w=False, m=False, y=False):\n",
    "    \"\"\"\n",
    "        :param\n",
    "            - df : DataFrame\n",
    "            - w : Boolean True if we want a week period, False otherwise\n",
    "            - m : Boolean True if we want a month period, False otherwise\n",
    "            - y : Boolean True if we want a year period, False otherwise\n",
    "\n",
    "        :return Average daily return calculated following the perdio choose\n",
    "    \"\"\"\n",
    "    list = []\n",
    "    if w:\n",
    "        dates = df_zoom.select(weekofyear(df_zoom['Date'])).collect()\n",
    "        for i in range(len(dates)):\n",
    "            if dates[i][0] not in list:\n",
    "                list.append(dates[i][0])\n",
    "    else:\n",
    "        dates = df.select('Date').collect() #convert column to list\n",
    "        for date in dates:\n",
    "            if y:\n",
    "                if date[0].year not in list:\n",
    "                    list.append(date[0].year)\n",
    "            else:\n",
    "                if date[0].month not in list:\n",
    "                    list.append(date[0].month)\n",
    "\n",
    "    list.sort()\n",
    "\n",
    "    for i in list:\n",
    "        if y: df_filtered = df.filter(year(df['Date']) == i)\n",
    "        elif m: df_filtered = df.filter(month(df['Date']) == i)\n",
    "        else: df_filtered = df.filter(weekofyear(df['Date']) == i)\n",
    "        print(\"Average daily return in\", i)\n",
    "        df_filtered.agg(mean(df_filtered['Close']- df_filtered['Open'])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main class\n",
    "Maintenant que nous avons toutes nos fonctions, nous pouvons faire une classe main qui les appelera toute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
